{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m/        cleaned.txt         \u001b[34mmodel\u001b[m\u001b[m/              view_logs.py\r\n",
      "\u001b[34mbooks\u001b[m\u001b[m/              \u001b[34mdata\u001b[m\u001b[m/               steps.sh            view_outputs.py\r\n",
      "char_rnn_kur.ipynb  make_data.py        view_data.py        vocab.py\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kurfile.yaml  \u001b[34mlast.w.kur\u001b[m\u001b[m/   \u001b[34mlog\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Natsume/Downloads/kur_road/kur/examples/language-model/model\n"
     ]
    }
   ],
   "source": [
    "%cd model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pycat kurfile.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_rnn_kur_quick.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_rnn_kur_quick.yml\n",
    "---\n",
    "\n",
    "settings:\n",
    "\n",
    "  vocab:\n",
    "    size: 30\n",
    "\n",
    "  rnn:\n",
    "    size: 128\n",
    "    depth: 3\n",
    "\n",
    "model:\n",
    "  - input: in_seq\n",
    "\n",
    "  - for:\n",
    "      range: \"{{ rnn.depth - 1 }}\"\n",
    "      iterate:\n",
    "        - recurrent:\n",
    "            size: \"{{ rnn.size }}\"\n",
    "            type: gru\n",
    "            sequence: yes               \n",
    "            bidirectional: no\n",
    "        - batch_normalization\n",
    "\n",
    "  - recurrent:\n",
    "      size: \"{{ rnn.size }}\"\n",
    "      type: gru\n",
    "      sequence: no\n",
    "      bidirectional: no\n",
    "\n",
    "  - dense: \"{{ vocab.size }}\"\n",
    "\n",
    "  - activation: softmax\n",
    "\n",
    "  - output: out_char\n",
    "\n",
    "loss:\n",
    "  - target: out_char\n",
    "    name: categorical_crossentropy\n",
    "\n",
    "train:\n",
    "  data:\n",
    "    - jsonl: ../data/train.jsonl\n",
    "        \n",
    "  provider:\n",
    "    num_batches: 2\n",
    "  epochs: 1\n",
    "  weights:\n",
    "    initial: inital.w.kur\n",
    "    best: best.w.kur\n",
    "    last: last.w.kur\n",
    "\n",
    "#   log: log\n",
    "\n",
    "validate:\n",
    "  data:\n",
    "    - jsonl: ../data/validate.jsonl\n",
    "    \n",
    "  provider: \n",
    "    num_batches: 1\n",
    "  weights:\n",
    "    initial: inital.w.kur\n",
    "    best: best.w.kur\n",
    "    last: last.w.kur\n",
    "\n",
    "\n",
    "test:\n",
    "  data:\n",
    "    - jsonl: ../data/test.jsonl\n",
    "  weights:\n",
    "    initial: inital.w.kur\n",
    "    best: best.w.kur\n",
    "    last: last.w.kur\n",
    "\n",
    "\n",
    "evaluate:\n",
    "  data:\n",
    "    - jsonl: ../data/evaluate.jsonl\n",
    "  weights: best.w.kur\n",
    "\n",
    "  destination: output.pkl\n",
    "    \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-03 21:06:37,503 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_kur_quick.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:06:37,516 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,516 kur.kurfile:784]\u001b[0m Parsing Kurfile section: settings\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,516 kur.kurfile:784]\u001b[0m Parsing Kurfile section: train\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,520 kur.kurfile:784]\u001b[0m Parsing Kurfile section: validate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,522 kur.kurfile:784]\u001b[0m Parsing Kurfile section: test\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,524 kur.kurfile:784]\u001b[0m Parsing Kurfile section: evaluate\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,527 kur.containers.layers.placeholder:63]\u001b[0m Using short-hand name for placeholder: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,528 kur.containers.layers.placeholder:97]\u001b[0m Placeholder \"in_seq\" has a deferred shape.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,535 kur.containers.layers.output:50]\u001b[0m Using short-hand name for output: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:06:37,536 kur.kurfile:784]\u001b[0m Parsing Kurfile section: loss\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:21:52,176 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:21:52,196 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:10,681 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 32\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:10,681 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:10,686 kur.backend.backend:187]\u001b[0m Using backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:10,687 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:10,687 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:10,687 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:10,688 kur.backend.keras_backend:124]\u001b[0m Using the system-default Keras backend.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:10,689 kur.backend.keras_backend:189]\u001b[0m Overriding environmental variables: {'KERAS_BACKEND': None, 'THEANO_FLAGS': None, 'TF_CPP_MIN_LOG_LEVEL': '1'}\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:11,798 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:11,799 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:11,799 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,799 kur.model.model:272]\u001b[0m Assembled Node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,799 kur.model.model:274]\u001b[0m   Uses: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,799 kur.model.model:276]\u001b[0m   Used by: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:277]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:274]\u001b[0m   Uses: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:274]\u001b[0m   Uses: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:276]\u001b[0m   Used by: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:276]\u001b[0m   Used by: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:272]\u001b[0m Assembled Node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:274]\u001b[0m   Uses: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:276]\u001b[0m   Used by: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:277]\u001b[0m   Aliases: ..batch_normalization.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:272]\u001b[0m Assembled Node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,800 kur.model.model:274]\u001b[0m   Uses: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:276]\u001b[0m   Used by: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:277]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:272]\u001b[0m Assembled Node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:274]\u001b[0m   Uses: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:276]\u001b[0m   Used by: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:277]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:272]\u001b[0m Assembled Node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:274]\u001b[0m   Uses: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:276]\u001b[0m   Used by: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:277]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:272]\u001b[0m Assembled Node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:274]\u001b[0m   Uses: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,801 kur.model.model:276]\u001b[0m   Used by: \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:277]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:11,803 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:311]\u001b[0m Building node: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:312]\u001b[0m   Aliases: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.containers.layers.placeholder:117]\u001b[0m Creating placeholder for \"in_seq\" with data type \"float32\".\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:125]\u001b[0m Trying to infer shape for input \"in_seq\"\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.model.model:143]\u001b[0m Inferred shape for input \"in_seq\": (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,803 kur.containers.layers.placeholder:127]\u001b[0m Inferred shape: (30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,808 kur.model.model:382]\u001b[0m   Value: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,809 kur.model.model:311]\u001b[0m Building node: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,809 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,809 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:11,809 kur.model.model:315]\u001b[0m   - in_seq: in_seq\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,558 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,558 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,558 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,558 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,559 kur.model.model:315]\u001b[0m   - ..recurrent.0: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,577 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,577 kur.model.model:311]\u001b[0m Building node: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,577 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,577 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,577 kur.model.model:315]\u001b[0m   - ..batch_normalization.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,628 kur.model.model:382]\u001b[0m   Value: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,628 kur.model.model:311]\u001b[0m Building node: ..batch_normalization.1\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,628 kur.model.model:312]\u001b[0m   Aliases: ..batch_normalization.1, ..for.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,630 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,630 kur.model.model:315]\u001b[0m   - ..recurrent.1: DimShuffle{1,0,2}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,649 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,649 kur.model.model:311]\u001b[0m Building node: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,649 kur.model.model:312]\u001b[0m   Aliases: ..recurrent.2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,649 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,649 kur.model.model:315]\u001b[0m   - ..batch_normalization.1: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,698 kur.model.model:382]\u001b[0m   Value: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,699 kur.model.model:311]\u001b[0m Building node: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,699 kur.model.model:312]\u001b[0m   Aliases: ..dense.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,699 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,699 kur.model.model:315]\u001b[0m   - ..recurrent.2: Subtensor{int64}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,702 kur.model.model:382]\u001b[0m   Value: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,702 kur.model.model:311]\u001b[0m Building node: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,702 kur.model.model:312]\u001b[0m   Aliases: ..activation.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,702 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,702 kur.model.model:315]\u001b[0m   - ..dense.0: Elemwise{add,no_inplace}.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:311]\u001b[0m Building node: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:312]\u001b[0m   Aliases: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:313]\u001b[0m   Inputs:\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:315]\u001b[0m   - ..activation.0: Softmax.0\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,703 kur.model.model:382]\u001b[0m   Value: Softmax.0\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:12,703 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:12,703 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:12,704 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: inital.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:12,704 kur.model.executor:302]\u001b[0m No log specified, so no historical loss information is available.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:12,704 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,705 kur.model.executor:353]\u001b[0m Epoch handling mode: additional\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,705 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:12,705 kur.backend.keras_backend:527]\u001b[0m Instantiating a Keras model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m Layer (type)                     Output Shape          Param #     Connected to                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m in_seq (InputLayer)              (None, 30, 30)        0                                            \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..recurrent.0 (GRU)              (None, 30, 128)       61056       in_seq[0][0]                     \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.0 (BatchNo (None, 30, 128)       512         ..recurrent.0[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..recurrent.1 (GRU)              (None, 30, 128)       98688       ..batch_normalization.0[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..batch_normalization.1 (BatchNo (None, 30, 128)       512         ..recurrent.1[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..recurrent.2 (GRU)              (None, 128)           98688       ..batch_normalization.1[0][0]    \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,004 kur.backend.keras_backend:538]\u001b[0m ..dense.0 (Dense)                (None, 30)            3870        ..recurrent.2[0][0]              \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m ..activation.0 (Activation)      (None, 30)            0           ..dense.0[0][0]                  \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m ====================================================================================================\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m Total params: 263,326\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m Trainable params: 262,814\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m Non-trainable params: 512\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m ____________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,005 kur.backend.keras_backend:538]\u001b[0m \u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,006 kur.backend.keras_backend:576]\u001b[0m Assembling a training function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:14,011 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,091 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,091 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,091 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'func': <keras.backend.theano_backend.Function object at 0x111c637f0>, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}, 'shapes': {'input': [(None, 30, 30), (None, None)]}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,092 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,092 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:22:30,113 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,114 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,115 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,195 kur.providers.provider:144]\u001b[0m Data source \"in_seq\": entries=1330234, shape=(30, 30)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:22:30,195 kur.providers.provider:144]\u001b[0m Data source \"out_char\": entries=1330234, shape=(30,)\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=N/A:   0%|                          | 0/64 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-03 21:22:30,209 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,814 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,816 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,817 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,819 kur.providers.batch_provider:156]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,829 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:18,959 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.866:  50%|███████▌       | 32/64 [00:48<00:48,  1.52s/samples]\u001b[1;34m[DEBUG 2017-03-03 21:23:18,960 kur.model.executor:578]\u001b[0m Training on batch...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,058 kur.model.executor:597]\u001b[0m Finished training on batch.\u001b[0m\n",
      "Epoch 1/1, loss=3.812: 100%|███████████████| 64/64 [00:48<00:00,  1.31samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:19,063 kur.model.executor:464]\u001b[0m Training loss: 3.812\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:19,065 kur.model.executor:471]\u001b[0m Saving best historical training weights: best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,065 kur.model.executor:430]\u001b[0m Saving weights to: best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,067 kur.model.model:213]\u001b[0m Saving model weights to: best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,091 kur.model.executor:101]\u001b[0m Recompiling the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,091 kur.backend.keras_backend:543]\u001b[0m Reusing an existing model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,091 kur.backend.keras_backend:560]\u001b[0m Assembling a testing function from the model.\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:19,108 kur.backend.keras_backend:509]\u001b[0m Adding additional inputs: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,424 kur.backend.keras_backend:599]\u001b[0m Additional inputs for log functions: out_char\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,424 kur.backend.keras_backend:616]\u001b[0m Expected input shapes: in_seq=(None, 30, 30), out_char=(None, None)\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,424 kur.backend.keras_backend:634]\u001b[0m Compiled model: {'func': <keras.backend.theano_backend.Function object at 0x107f75780>, 'names': {'output': ['..activation.0', 'out_char'], 'input': ['in_seq', 'out_char']}, 'shapes': {'input': [(None, 30, 30), (None, None)]}}\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,425 kur.providers.batch_provider:57]\u001b[0m Batch size set to: 2\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,425 kur.providers.batch_provider:102]\u001b[0m Maximum number of batches set to: 1\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:21,445 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,445 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:21,445 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=N/A:   0%|                         | 0/32 [00:00<?, ?samples/s]\u001b[1;34m[DEBUG 2017-03-03 21:23:21,478 kur.providers.shuffle_provider:184]\u001b[0m Shuffling...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:23,961 kur.providers.batch_provider:139]\u001b[0m Preparing next batch of data...\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:23,962 kur.providers.batch_provider:204]\u001b[0m Next batch of data has been prepared.\u001b[0m\n",
      "Validating, loss=3.655: 100%|██████████████| 32/32 [00:02<00:00, 12.70samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:23,996 kur.model.executor:197]\u001b[0m Validation loss: 3.655\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:23,997 kur.model.executor:413]\u001b[0m Saving best historical validation weights: best.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:23,997 kur.model.executor:442]\u001b[0m Recent weight file seems the same as the soon-to-be-saved file. Skipping: best.w.kur\u001b[0m\n",
      "Completed 1 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-03 21:23:23,997 kur.model.executor:235]\u001b[0m Saving most recent weights: last.w.kur\u001b[0m\n",
      "\u001b[1;34m[DEBUG 2017-03-03 21:23:23,997 kur.model.model:213]\u001b[0m Saving model weights to: last.w.kur\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!kur -vv train char_rnn_kur_quick.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-03 22:35:58,437 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_kur_quick.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:35:58,449 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:21,669 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:21,670 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:21,670 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:22,711 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:22,712 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:22,712 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:22,712 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:23,706 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:23,706 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:36:26,428 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Evaluating: 100%|██████████████████| 83140/83140 [00:41<00:00, 2018.06samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-03 22:37:07,814 kur.model.hooks.output_hook:40]\u001b[0m Saving model output as pickle: output.pkl\u001b[0m\n",
      "CPU times: user 983 ms, sys: 279 ms, total: 1.26 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v evaluate char_rnn_kur_quick.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37m[INFO 2017-03-03 22:37:24,029 kur.kurfile:699]\u001b[0m Parsing source: char_rnn_kur_quick.yml, included by top-level.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:37:24,041 kur.kurfile:82]\u001b[0m Parsing Kurfile...\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:39,826 kur.backend.backend:80]\u001b[0m Creating backend: keras\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:39,835 kur.backend.backend:83]\u001b[0m Backend variants: none\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:39,835 kur.backend.keras_backend:122]\u001b[0m No particular backend for Keras has been requested.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:40,943 kur.backend.keras_backend:195]\u001b[0m Keras is loaded. The backend is: theano\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:40,944 kur.model.model:260]\u001b[0m Enumerating the model containers.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:40,944 kur.model.model:265]\u001b[0m Assembling the model dependency graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:40,944 kur.model.model:280]\u001b[0m Connecting the model graph.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:41,853 kur.model.model:284]\u001b[0m Model inputs:  in_seq\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:41,853 kur.model.model:285]\u001b[0m Model outputs: out_char\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:41,854 kur.kurfile:357]\u001b[0m Ignoring missing initial weights: inital.w.kur. If this is undesireable, set \"must_exist\" to \"yes\" in the approriate \"weights\" section.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:41,855 kur.model.executor:302]\u001b[0m No log specified, so no historical loss information is available.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:41,855 kur.model.executor:329]\u001b[0m No previous epochs.\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:52:59,311 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "\n",
      "Epoch 1/1, loss=3.316: 100%|███████████████| 64/64 [01:02<00:00,  1.37s/samples]\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:02,039 kur.model.executor:464]\u001b[0m Training loss: 3.316\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:02,051 kur.model.executor:471]\u001b[0m Saving best historical training weights: best.w.kur\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:06,777 kur.backend.keras_backend:666]\u001b[0m Waiting for model to finish compiling...\u001b[0m\n",
      "Validating, loss=3.348: 100%|██████████████| 32/32 [00:02<00:00, 11.61samples/s]\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:09,558 kur.model.executor:197]\u001b[0m Validation loss: 3.348\u001b[0m\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:09,558 kur.model.executor:413]\u001b[0m Saving best historical validation weights: best.w.kur\u001b[0m\n",
      "Completed 1 epochs.\n",
      "\u001b[1;37m[INFO 2017-03-03 22:54:09,559 kur.model.executor:235]\u001b[0m Saving most recent weights: last.w.kur\u001b[0m\n",
      "CPU times: user 11.9 s, sys: 3.27 s, total: 15.2 s\n",
      "Wall time: 16min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!kur -v train char_rnn_kur_quick.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
